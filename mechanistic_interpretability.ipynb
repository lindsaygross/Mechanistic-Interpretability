{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw5uQnGcBeak"
      },
      "source": [
        "\n",
        "\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lindsaygross/mechanistic-interpretability/blob/main/mechanistic-interpretability.ipynb)\n",
        "\n",
        "\n",
        "# Mechanistic Interpretability Assignment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ho3hsFc0CLYv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np, random, string, matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "torch.manual_seed(0); random.seed(0); np.random.seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIzmdBdGCN0q"
      },
      "source": [
        "### Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xj3f7FGPCQkf"
      },
      "outputs": [],
      "source": [
        "ALPHABET = list(string.ascii_uppercase)\n",
        "VOWELS = set(\"AEIOU\")\n",
        "SEQ_LEN = 4\n",
        "ALPH = 26\n",
        "INPUT_DIM = SEQ_LEN * ALPH\n",
        "\n",
        "def one_hot_word(word):\n",
        "    x = np.zeros((SEQ_LEN, ALPH), dtype=np.float32)\n",
        "    for i,ch in enumerate(word):\n",
        "        x[i, ord(ch)-65] = 1.0\n",
        "    return x.reshape(-1)\n",
        "\n",
        "def make_dataset(n):\n",
        "    X, y, words = [], [], []\n",
        "    for _ in range(n):\n",
        "        w = \"\".join(random.choice(ALPHABET) for _ in range(SEQ_LEN))\n",
        "        X.append(one_hot_word(w))\n",
        "        y.append(int(any(c in VOWELS for c in w)))\n",
        "        words.append(w)\n",
        "    X = torch.tensor(np.stack(X), dtype=torch.float32)\n",
        "    y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "    return X, y, words\n",
        "\n",
        "X_train, y_train, _ = make_dataset(6000)\n",
        "X_val,   y_val,   _ = make_dataset(1000)\n",
        "X_test,  y_test,  _ = make_dataset(1000)\n",
        "\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_val,   y_val   = X_val.to(device),   y_val.to(device)\n",
        "X_test,  y_test  = X_test.to(device),  y_test.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL9twmqeCRL7"
      },
      "source": [
        "### Build a Tiny MLP (multilayer perceptron) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NVlI54ibCbaf"
      },
      "outputs": [],
      "source": [
        "ALPHABET = list(string.ascii_uppercase)\n",
        "VOWELS = set(\"AEIOU\")\n",
        "SEQ_LEN = 4\n",
        "ALPH = 26\n",
        "INPUT_DIM = SEQ_LEN * ALPH\n",
        "\n",
        "def one_hot_word(word):\n",
        "    x = np.zeros((SEQ_LEN, ALPH), dtype=np.float32)\n",
        "    for i,ch in enumerate(word):\n",
        "        x[i, ord(ch)-65] = 1.0\n",
        "    return x.reshape(-1)\n",
        "\n",
        "def make_dataset(n):\n",
        "    X, y, words = [], [], []\n",
        "    for _ in range(n):\n",
        "        w = \"\".join(random.choice(ALPHABET) for _ in range(SEQ_LEN))\n",
        "        X.append(one_hot_word(w))\n",
        "        y.append(int(any(c in VOWELS for c in w)))\n",
        "        words.append(w)\n",
        "    X = torch.tensor(np.stack(X), dtype=torch.float32)\n",
        "    y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "    return X, y, words\n",
        "\n",
        "X_train, y_train, _ = make_dataset(6000)\n",
        "X_val,   y_val,   _ = make_dataset(1000)\n",
        "X_test,  y_test,  _ = make_dataset(1000)\n",
        "\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_val,   y_val   = X_val.to(device),   y_val.to(device)\n",
        "X_test,  y_test  = X_test.to(device),  y_test.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define neural network class\n",
        "class VowelMLP(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_DIM, hidden=12):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, 1)\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        out = torch.sigmoid(self.fc2(h))\n",
        "        return out, h\n",
        "\n",
        "model = VowelMLP().to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
        "loss_fn = nn.BCELoss()\n"
      ],
      "metadata": {
        "id": "VJepqNhZ8V_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13mAiuq4CkXu"
      },
      "source": [
        "### TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ukiy5-iS_96x",
        "outputId": "9cda8bca-6354-4cc6-93fa-7db84c1ac7c9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4266782208.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "def batches(X, y, bs=256):\n",
        "    idx = torch.randperm(len(X), device=X.device)\n",
        "    for i in range(0, len(idx), bs):\n",
        "        j = idx[i:i+bs]\n",
        "        yield X[j], y[j]\n",
        "\n",
        "for epoch in range(8):\n",
        "    model.train()\n",
        "    for xb, yb in batches(X_train, y_train):\n",
        "        opt.zero_grad()\n",
        "        p, _ = model(xb)\n",
        "        loss = loss_fn(p, yb)\n",
        "        loss.backward(); opt.step()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pv, _ = model(X_val)\n",
        "    acc = ((pv>0.5).float()==y_val).float().mean().item()\n",
        "    print(f\"epoch {epoch+1} val_acc {acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ],
      "metadata": {
        "id": "GEXcBksU7cOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pt, H_test = model(X_test)\n",
        "yhat = (pt>0.5).float()\n",
        "test_acc = (yhat==y_test).float().mean().item()\n",
        "print(\"test_acc\", round(test_acc,3))\n"
      ],
      "metadata": {
        "id": "lJ0qbXMo7bqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore internals"
      ],
      "metadata": {
        "id": "aZybO1RW7e6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "H = H_test.detach().cpu().numpy()\n",
        "yv = y_test.cpu().numpy().ravel().astype(int)\n",
        "avg_no  = H[yv==0].mean(axis=0)\n",
        "avg_yes = H[yv==1].mean(axis=0)\n",
        "\n",
        "plt.figure(figsize=(7,3))\n",
        "i = np.arange(len(avg_no))\n",
        "plt.bar(i-0.2, avg_no, width=0.4, label=\"no vowel\")\n",
        "plt.bar(i+0.2, avg_yes, width=0.4, label=\"has vowel\")\n",
        "plt.xlabel(\"hidden neuron\"); plt.ylabel(\"avg activation\"); plt.legend(); plt.title(\"Hidden activation by class\")\n",
        "plt.show()\n",
        "\n",
        "key_neuron = int(np.argmax(avg_yes - avg_no))\n",
        "print(\"candidate vowel neuron:\", key_neuron)\n"
      ],
      "metadata": {
        "id": "V9JpsixB7hut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Letter selectivity"
      ],
      "metadata": {
        "id": "IAziRwTu7jCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_single_letter(letter, pos):\n",
        "    x = np.zeros((SEQ_LEN, ALPH), dtype=np.float32)\n",
        "    x[pos, ord(letter)-65] = 1.0\n",
        "    return torch.tensor(x.reshape(-1), dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "acts = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for L in ALPHABET:\n",
        "        pos_acts = []\n",
        "        for pos in range(SEQ_LEN):\n",
        "            _, h = model(encode_single_letter(L, pos))\n",
        "            pos_acts.append(h[0, key_neuron].item())\n",
        "        acts.append(np.mean(pos_acts))\n",
        "acts = np.array(acts)\n",
        "\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.bar(range(26), acts)\n",
        "plt.xticks(range(26), ALPHABET, rotation=90)\n",
        "plt.ylabel(f\"activation (neuron {key_neuron})\"); plt.title(\"Letter selectivity\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XEDYWP4s7kUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy drop"
      ],
      "metadata": {
        "id": "Jdg6oPcf7mRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ablate_accuracy(neuron_idx):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        h = F.relu(model.fc1(X_test))\n",
        "        h[:, neuron_idx] = 0.0\n",
        "        p = torch.sigmoid(model.fc2(h))\n",
        "        return ((p>0.5).float()==y_test).float().mean().item()\n",
        "\n",
        "baseline = test_acc\n",
        "drop_per_neuron = []\n",
        "for j in range(model.fc1.out_features):\n",
        "    acc_j = ablate_accuracy(j)\n",
        "    drop_per_neuron.append(baseline - acc_j)\n",
        "\n",
        "plt.figure(figsize=(7,3))\n",
        "plt.bar(range(len(drop_per_neuron)), drop_per_neuron)\n",
        "plt.xlabel(\"hidden neuron\"); plt.ylabel(\"acc drop vs baseline\")\n",
        "plt.title(\"Ablation impact\")\n",
        "plt.show()\n",
        "\n",
        "print(\"worst ablation neuron:\", int(np.argmax(drop_per_neuron)))\n"
      ],
      "metadata": {
        "id": "rp0v3aVT7n1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0puB8G1Frlb"
      },
      "source": [
        "#### **What does this plot show us?**\n",
        "\n",
        "##### *Neuron Specialization*\n",
        "Look for rows (neurons) that are brightest at specific counts.\n",
        "\n",
        "If a neuron appears consistently dark, that could mean it is unused or suppressed.\n",
        "\n",
        "##### *Monotonic Trends*\n",
        "If neurons show a gradual increase in activation with count — they may be learning a linear or thresholded count pattern.\n",
        "\n",
        "That’s a great example of linearly increasing features often discussed in mechanistic interpretability (e.g. linear probe-ability).\n",
        "\n",
        "##### *Redundant or Silent Units*\n",
        "Some neurons may appear flatlined which possibly means they are unused due to redundancy. *What happens if you remove the neuron entirely?*\n",
        "\n",
        "##### *Activation Superposition*\n",
        "Some neurons activate across a range of counts, possibly blending multiple input features — this ties into superposition theory (see: Toy Models of Superposition)\n",
        "\n",
        "_____________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcUgRnPhGkzj"
      },
      "source": [
        "#### **What does the ablation tell us?**\n",
        "\n",
        "If the prediction changes significantly after zeroing out a neuron, that neuron likely plays an important functional role in the model’s decision!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9a6TAe-FlFo"
      },
      "source": [
        "### Neuron Ablation (sweep of neurons)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYLx9tLBGuUq"
      },
      "source": [
        "#### **What does the sweep of neuron ablations tell us?**\n",
        "\n",
        "Which neurons had the most impact on predictions?\n",
        "\n",
        "Are there redundant neurons (zeroing them causes no change)?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}